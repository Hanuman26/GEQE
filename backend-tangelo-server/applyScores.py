import syssys.path.append(".")import confimport jsonimport tangeloimport osimport time@tangelo.restfuldef get(filePath='./',filePolygon='polyFile.txt', fileAppOut='appliedScores.csv', fScoreThresh='0.005', dataSet='', useML="true", useBayes="false", nFeatures="300", custStopWord=""):    if filePath[-1] != '/': filePath = filePath+"/"    timeStamp = str(time.time())    ssName = filePath + "applyScores_" + timeStamp + ".sh"    useML = "true" == useML    # load the correct dataset by name from the data set config file    confObj = conf.get()    dataSetDict = confObj['datasets']    sparkSettings = confObj['spark']    if dataSet not in dataSetDict:        raise ValueError("Data set not found in conf. "+dataSet)    dataSetObj = dataSetDict[dataSet]    dataSetName = dataSet    dataSetPath = dataSetObj['path']    dataSetType = dataSetObj['type']    cleanCustStop = custStopWord.replace(" ","").replace("\"","\\\"")    algorithm = "aynWald_fsp.py" if useML else "findSimilarPlaces.py"    if 'local' == confObj['deploy-mode'] or 'cluster' == confObj['deploy-mode']:        # deploy using spark submit        launchCommand = [ sparkSettings['SPARK_SUBMIT_PATH']+"spark-submit",                         "--py-files",                         "lib/shapeReader.py,lib/pointClass.py,lib/fspLib.py",                         "--executor-memory "+sparkSettings['EXECUTOR_MEMORY'],                         "--driver-memory "+sparkSettings['DRIVER_MEMORY'],                         "--total-executor-cores "+sparkSettings['TOTAL_EXECUTOR_CORES'],                         algorithm,                         sparkSettings['SPARK_MASTER'],                         dataSetPath,                         "inputFiles/"+filePolygon,                         "dict_"+fileAppOut,                         filePath+"/scoreFiles/"+fileAppOut,                         "-jobNm onlineApplication",                         "-datTyp "+str(dataSetType),                         "-sThresh "+fScoreThresh,                         " -sCustStop \"" + cleanCustStop + '\"',                         "--sit"        ]        if useML: launchCommand.append(" -nFeatures "+ nFeatures)        if useML and useBayes != "false": launchCommand.append("--useBayes")        launchString = ' '.join(launchCommand)        f = open(ssName, 'w')        f.write(launchString)        f.close()        os.system("chmod u+x " + ssName)        os.chdir(filePath)        os.system("nohup " + ssName + " >& consoleApply_" + timeStamp + ".txt &")        return "0"    elif 's3-emr' == confObj['deploy-mode']:        # deploy with to EMR cluster with S3/SQS        import awsutil        if filePath[-1] != '/': filePath = filePath+"/"        runConf = {}        runConf['spark'] = sparkSettings        runConf['dataSetPath'] = dataSetPath        runConf['dataSetType'] = dataSetType        runConf['-sThresh'] = fScoreThresh        runConf['-sCustStop'] = cleanCustStop        runConf['algorithim'] = algorithm        runConf['useML'] = useML        runConf['useBayes'] = useBayes != "false"        runConf['-nFeatures'] = nFeatures        jobname = awsutil.createJob(runConf,                          filePath+'inputFiles/'+filePolygon,                          filePath+'scoreFiles/'+fileAppOut,                          filePath+'dictFiles/dict_'+fileAppOut)        return jobname