import syssys.path.append(".")from decorators import allow_all_originsfrom decorators import validate_userimport confimport jsonimport tangeloimport osimport timeimport datetimeimport commandlineLauncherdef generate_job_name(scoreFile):    """ generate a job id based on the current time """    filename = scoreFile if ('/' not in scoreFile) else scoreFile.split('/')[-1]    return 'job_'+filename@tangelo.restful@allow_all_origins@validate_userdef get(user="demo",filePolygon='polyFile.txt',fileAppOut='appliedScores',        fScoreThresh='0.005',        dataSet='',        useTime="false", nFeatures="300", custStopWord=""):    useTime = ("true" == useTime) or ("True" == useTime)    # load the correct dataset by name from the data set config file    confObj = conf.get()    dataSetDict = confObj['datasets']    sparkSettings = confObj['spark']    if dataSet not in dataSetDict:        raise ValueError("Data set not found in conf. "+dataSet)    dataSetObj = dataSetDict[dataSet]    dataSetName = dataSet    dataSetPath = dataSetObj['path']    dataSetType = dataSetObj['type']    filePath = confObj['root_data_path'] +'/' +user    cleanCustStop = custStopWord.replace(" ","").replace("\"","\\\"")    algorithm = "aynWald_fsp.py" if not useTime else "timeWald_fsp.py"    # deploy using spark submit    launchCommand = [ sparkSettings['SPARK_SUBMIT_PATH']+"spark-submit"]    launchCommand.extend(sparkSettings['SPARK_OPTIONS'])    launchCommand.extend([algorithm,dataSetPath])    if  'aws-emr' == confObj['deploy-mode'] :        launchCommand.append('poly')        launchCommand.append('dict')        launchCommand.append('score')    else:        launchCommand.append(filePath+"/inputFiles/"+filePolygon)        launchCommand.append(filePath+"/dictFiles/dict_"+fileAppOut)        launchCommand.append(filePath+"/scoreFiles/"+fileAppOut)    launchCommand.extend(["-jobNm","geqe-applyScores-"+algorithm,                         "-datTyp",str(dataSetType),                         "-sThresh",fScoreThresh,                         "-nFeatures", nFeatures,                         "-sCustStop", "\"" + cleanCustStop + '\"',                         "--sit"    ])    jobname = generate_job_name(fileAppOut)    # write the job file    jobHeader = {        'polygonFile' : filePolygon,        'scoreFile': fileAppOut,        'dataSetName': dataSetName,        'jobname': jobname,        'args' : launchCommand    }    with open(filePath+'/jobFiles/'+jobname,'w') as jobFile:        jobFile.write(json.dumps(jobHeader))    if 'local' == confObj['deploy-mode'] or 'cluster' == confObj['deploy-mode']:        workdir = confObj['workdir']        commandlineLauncher.runCommand(jobname,launchCommand,workdir)        return jobname    elif 'aws-emr' == confObj['deploy-mode']:        import awsutil        bucket = confObj['s3-bucket']        jobconf = {}        jobconf['run_command'] = launchCommand        jobconf['dict_save_path'] =  filePath+'/dictFiles/dict_'+fileAppOut        jobconf['score_save_path'] = filePath+'/scoreFiles/'+fileAppOut        awsutil.submitJob(jobname,jobconf,filePath+'/inputFiles/'+filePolygon,bucket)        tangelo.log("Submited job to aws. "+jobname)        return jobname    else:        raise Exception("Invalid deploy mode in conf.json.  should be local cluster or aws-emr")