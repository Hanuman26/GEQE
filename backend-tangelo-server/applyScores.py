import syssys.path.append(".")from decorators import allow_all_originsimport confimport jsonimport tangeloimport osimport timeimport datetimeimport commandlineLauncherdef generate_job_name():    """ generate a job id based on the current time """    return 'job_applyScores_'+str(datetime.datetime.now()).replace(' ','_')@tangelo.restful@allow_all_originsdef get(filePath='./',filePolygon='polyFile.txt',fileAppOut='appliedScores.csv',        fScoreThresh='0.005',        dataSet='',        useTime="false", nFeatures="300", custStopWord=""):    if filePath[-1] != '/': filePath = filePath+"/"    timeStamp = str(time.time())    ssName = filePath + "applyScores_" + timeStamp + ".sh"    useTime = ("true" == useTime) or ("True" == useTime)    # load the correct dataset by name from the data set config file    confObj = conf.get()    dataSetDict = confObj['datasets']    sparkSettings = confObj['spark']    if dataSet not in dataSetDict:        raise ValueError("Data set not found in conf. "+dataSet)    dataSetObj = dataSetDict[dataSet]    dataSetName = dataSet    dataSetPath = dataSetObj['path']    dataSetType = dataSetObj['type']    cleanCustStop = custStopWord.replace(" ","").replace("\"","\\\"")    algorithm = "aynWald_fsp.py" if useTime else "timeWald_fsp.py"    # deploy using spark submit    launchCommand = [ sparkSettings['SPARK_SUBMIT_PATH']+"spark-submit"]    launchCommand.extend(sparkSettings['SPARK_OPTIONS'])    launchCommand.extend([algorithm,dataSetPath])    if  'aws-emr' == confObj['deploy-mode'] :        launchCommand.append('poly')        launchCommand.append('dict')        launchCommand.append('score')    else:        launchCommand.append("inputFiles/"+filePolygon)        launchCommand.append("dict_"+fileAppOut)        launchCommand.append(filePath+"/scoreFiles/"+fileAppOut)    launchCommand.extend(["-jobNm","geqe-applyScores-"+algorithm,                         "-datTyp",str(dataSetType),                         "-sThresh",fScoreThresh,                         "-nFeatures", nFeatures,                         "-sCustStop", "\"" + cleanCustStop + '\"',                         "--sit"    ])    if 'local' == confObj['deploy-mode'] or 'cluster' == confObj['deploy-mode']:        jobname = generate_job_name()        commandlineLauncher.runCommand(jobname,launchCommand,filePath)        return jobname    elif 'aws-emr' == confObj['deploy-mode']:        import awsutil        bucket = confObj['s3-bucket']        jobname = generate_job_name()        jobconf = {}        jobconf['run_command'] = launchCommand        jobconf['dict_save_path'] =  filePath+'dictFiles/dict_'+fileAppOut        jobconf['score_save_path'] = filePath+'scoreFiles/'+fileAppOut        awsutil.submitJob(jobname,jobconf,filePath+'inputFiles/'+filePolygon,bucket)        tangelo.log("Submited job to aws. "+jobname)        return jobname    else:        raise Exception("Invalid deploy mode in conf.json.  should be local cluster or aws-emr")