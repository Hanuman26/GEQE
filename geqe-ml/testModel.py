############# ############# ############# ############# ############## testModel# by JAG3## v1.0 - Cross Validate training sites############## ############# ############# ############# #############from pyspark import SparkConf, SparkContextfrom pyspark.sql import SQLContextfrom pyspark.sql.types import BooleanTypefrom pyspark.mllib.regression import LabeledPointfrom pyspark.mllib.tree import RandomForestimport sysimport argparseimport jsonimport codecssys.path.insert(0, './lib/')import aggregatedComparisonimport shapeReaderimport fspLibimport timedef testPoly(sqlContext, sc, lTest, lTrain, lStop):    #Partition data into 3 parts: train (positive examples), train (negative examples), and test (this polygon)    t1 = time.time()    bc_lTestPoly  = sc.broadcast(lTest)    bc_lTrainPoly = sc.broadcast(lTrain)    sqlContext.registerFunction("inRegionOfInterest", lambda lat,lon: fspLib.inROI(lat,lon,bc_lTrainPoly),returnType=BooleanType())    sqlContext.registerFunction("inRegionOfTest", lambda lat,lon: fspLib.inROI(lat,lon,bc_lTestPoly),returnType=BooleanType())    df1 = sqlContext.sql("SELECT * from records WHERE inRegionOfInterest(records.lat,records.lon)").cache()    df1.registerTempTable("df1")    nIn = df1.count()    dfAp = sqlContext.sql("SELECT * from records WHERE inRegionOfTest(records.lat,records.lon)").cache()    dfAp.registerTempTable("dfAp")    nAp = dfAp.count()    dfn1 = sqlContext.sql("SELECT * from records WHERE NOT inRegionOfInterest(records.lat,records.lon) AND NOT inRegionOfTest(records.lat, records.lon)").cache()    dfn1.registerTempTable("dfn1")    nOut = dfn1.count()    diff = time.time()-t1    print "Time to find in and out of ROI", diff    print "N in:", nIn, ", N out:", nOut, ", N ap:", nAp    if nAp==0:        return []    #format all data for use in ML model    t1 = time.time()    groupedIn = df1.map(lambda x: (x.key, [LabeledPoint(1.0, x.vector), x.lat, x.lon, x.size, x.binSize])).cache()    groupedOut = dfn1.map(lambda x: (x.key, [LabeledPoint(-1.0, x.vector), x.lat, x.lon, x.size, x.binSize])).cache()    mlApply = dfAp.map(lambda x: (x.key, [LabeledPoint(-1.0, x.vector), x.lat, x.lon, x.size, x.binSize])).cache()    scaleFactor = (10.*nIn)/float(nOut)    (junk, groupedUse) = groupedOut.randomSplit([1-scaleFactor,scaleFactor])    mlTrain = groupedIn.union(groupedUse)    if len(lStop) != 0:        mlTrain = mlTrain.map(lambda x: aggregatedComparison.removeStopWords(x,lStop))    mlTrain.cache()    nTotTrain = mlTrain.count()    mlApply.cache()    nApply = mlApply.count()    t2 = time.time()    print nTotTrain, "entries for training"    diff = t2-t1    print "Time to get data ready for model by time", diff    #train model    t1 = time.time()    model_Tree = RandomForest.trainRegressor(mlTrain.map(lambda x: x[1][0]), categoricalFeaturesInfo={}, numTrees=2000, featureSubsetStrategy="auto", impurity="variance", maxDepth=4, maxBins=32)    diff = time.time()-t1    print "Time to train model", diff    #apply model to polygon of interest, get results    t1 = time.time()    predictions_Tree = model_Tree.predict(mlApply.map(lambda x: x[1][0].features))    vecAndPredictions = mlApply.zip(predictions_Tree)    vecAndPredictions.cache()    vecAndPredictions.count()    results = vecAndPredictions.collect()    clusters = []    for point in results:        off = point[0][1][4]/2.        lat = point[0][1][1] + off if point[0][1][1] > 0 else point[0][1][1] - off        lon = point[0][1][2] + off if point[0][1][2] > 0 else point[0][1][2] - off        thisCluster = {"score":point[1], "dict":{}, "lat":lat, "lon":lon, "nTotal":point[0][1][3]}        thisCluster["poly"] = [[lat+off,lon+off],[lat+off,lon-off],[lat-off,lon-off],[lat-off,lon+off]]        clusters.append(thisCluster)    diff = time.time()-t1    print "Time to apply model and format results", diff    return clustersdef run(jobNm, sc, sqlContext, inputFile, lPolygon, dictFile,        inputPartitions=-1,        writeFileOutput=True,        strStop=''):    stopSet = set(strStop.split(',')) if strStop !='' else set()    t0 = time.time()    #Read in data, done once for all data    t1 = time.time()    records = aggregatedComparison.loadPoint(sc, sqlContext, inputFile, inputPartitions).cache()    nGoodTweets = records.count()    diff = time.time() - t1    print "Time to read in data:", diff    #Read in dictionary, once for all data    t1 = time.time()    revLookup = []    lStop = []    fDict = None    if dictFile[:3] == 's3:' or dictFile[:5] == 'hdfs:':        # read dict file from hdfs        fDict = sc.textFile(dictFile).collect()    else:        # read from local file        fDict = open(dictFile,"r")    for line in fDict:        terms = line.split("\t")        revLookup.append(terms[0])        if terms[0] in stopSet:            lStop.append(terms[1])    diff = time.time() - t1    print "Time to read in dict:", diff    #Itterate over the inputPolygons, train model on other areas,    #apply to polygon under inspection, then add them to the list    #of scored points.    clusters = []    for i in range(len(lPolygon)):        print "Testing site",lPolygon[i].name,"(",i,")"        lTest = [lPolygon[i]]        lTrain = lPolygon[:i]        lTrain.extend(lPolygon[i+1:])        clusters.extend(testPoly(sqlContext, sc, lTest, lTrain, lStop))    retDict = {"type":"place", "clusters":clusters, "modelDict":{}}    with codecs.open("scoreFiles/"+jobNm, encoding="utf-8",mode="wb") as fOut:        json.dump(retDict, fOut)    diff = time.time() - t0    print "<----------BOOM GOES THE DYNOMITE!---------->"    print "< total number of tweets:,", nGoodTweets    print "< total process Time:", diff    print "<------------------------------------------->"if __name__ == "__main__":    parser = argparse.ArgumentParser()    parser.add_argument("inputFile", help="Directory or file name (e.g. 'hdfs://domain.here.dev:/pathToData/")    parser.add_argument("polygonShapeFile", help="csv file specifying the bounding box for areas of interest")    parser.add_argument("jobNm", help="Application name, default = 'Find Similar Events'",default='findEvents')    parser.add_argument("-dictFile", help="Dictionary file to read in", default="dictFiles/dict_combinedIDF")    parser.add_argument("-partitions", help="repartition the input data set before processing.",type=int,default=-1)    parser.add_argument("-strStop", help="Comma delimited list of stop words to be removed from training", default="")    args = parser.parse_args()    inputFile = args.inputFile    shapeFile = args.polygonShapeFile    jobNm = args.jobNm    dictFile = args.dictFile    inputPartitions = args.partitions    strStop = args.strStop    conf = SparkConf().setAppName(jobNm)    sc = SparkContext(conf = conf)    sqlContext = SQLContext(sc)    #Create polygon list and broadcast variable based on it    lPolygon = shapeReader.readInShapeJson(shapeFile)    outList = []    run(jobNm, sc, sqlContext, inputFile, lPolygon, dictFile,        inputPartitions=inputPartitions,        strStop=strStop)